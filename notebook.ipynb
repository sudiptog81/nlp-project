{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install evaluate rouge_score bert_score bleuscore sacrebleu meteor bitsandbytes","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-05-09T19:59:04.480353Z","iopub.execute_input":"2025-05-09T19:59:04.480525Z","iopub.status.idle":"2025-05-09T20:01:19.750625Z","shell.execute_reply.started":"2025-05-09T19:59:04.480509Z","shell.execute_reply":"2025-05-09T20:01:19.749604Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ['HF_TOKEN'] = 'HF_TOKEN'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:01:19.751628Z","iopub.execute_input":"2025-05-09T20:01:19.751861Z","iopub.status.idle":"2025-05-09T20:01:19.756602Z","shell.execute_reply.started":"2025-05-09T20:01:19.751837Z","shell.execute_reply":"2025-05-09T20:01:19.755866Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import evaluate\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\nMODEL_MAP = {\n    'qwen': 'Qwen/Qwen2.5-1.5B-Instruct',\n    'opt': 'facebook/opt-iml-1.3b',\n    'llama': 'meta-llama/Llama-3.2-1B-Instruct'\n}\n\ndataset_map = {\n    'summarization': ('cnn_dailymail', '3.0.0'),\n    'qa': ('squad', None),\n    'paraphrase': ('quora', None)\n}\n\ndef format_prompt(model_key, task, item):\n    if task == 'summarization':\n        input_text = item['article']\n        instruction = \"Summarize the following article. Only provide the highlights from the given article.\"\n    elif task == 'qa':\n        input_text = f\"Context: {item['context']}\\nQuestion: {item['question']}\"\n        instruction = \"Answer the question based on the context.\"\n    elif task == 'paraphrase':\n        input_text = item['questions']['text'][0]\n        instruction = \"Paraphrase the following sentence. Only output a similar sentence or question.\"\n    else:\n        raise ValueError(\"Unknown task\")\n\n    if model_key == 'qwen':\n        return (\n            \"<|im_start|>system\\n\"\n            f\"{instruction}<|im_end|>\\n\"\n            \"<|im_start|>user\\n\"\n            f\"{input_text}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n    elif model_key == 'llama':\n        return (\n            \"<|start_header_id|>system<|end_header_id|>\\n\"\n            f\"{instruction}\\n\"\n            \"<|start_header_id|>user<|end_header_id|>\\n\"\n            f\"{input_text}\\n\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n        )\n    elif model_key == 'opt':\n        return (\n            f\"Instruction: {instruction}\\n\"\n            f\"Input: {input_text}\\n\"\n            \"Output: \"\n        )\n    else:\n        raise ValueError(f\"Unknown model key: {model_key}\")\n\ndef load_model(model_id, device='cuda'):\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    # model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n    tokenizer.pad_token = tokenizer.eos_token\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        device_map=device,\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n    )\n    return tokenizer, model\n\ndef generate_output(tokenizer, model, prompt, max_length=256):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=max_length)\n    return tokenizer.decode(outputs[0], skip_special_tokens=False)\n\ndef evaluate_summarization(preds, refs):\n    rouge = evaluate.load(\"rouge\")\n    results = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n    return results['rougeL']\n\ndef evaluate_qa(preds, refs):\n    rouge = evaluate.load(\"rouge\")\n    bertscore = evaluate.load(\"bertscore\")\n    rouge_score = rouge.compute(predictions=preds, references=refs)['rougeL']\n    bert_score = bertscore.compute(predictions=preds, references=refs, lang=\"en\")['f1']\n    return (rouge_score + sum(bert_score)/len(bert_score)) / 2\n\ndef evaluate_paraphrase(preds, refs):\n    bleu = evaluate.load(\"sacrebleu\")\n    meteor = evaluate.load(\"meteor\")\n    bleu_score = bleu.compute(predictions=preds, references=[[r] for r in refs])['score']\n    meteor_score = meteor.compute(predictions=preds, references=refs)['meteor']\n    return (bleu_score + meteor_score) / 2\n\ndef run_predictions(model_key, task, n_samples=100, split='validation', finetuned=False):\n    dataset_name, config = dataset_map[task]\n    dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n    if not finetuned:\n        tokenizer, model = load_model(MODEL_MAP[model_key])\n    else:\n        model_path = f\"{model_key}_{task}\"\n        tokenizer, model = load_model(model_path)\n        print(model_path, 'loaded')\n        \n    predictions, references = [], []\n    for item in tqdm(dataset):\n        prompt = format_prompt(model_key, task, item)\n\n        if task == 'summarization':\n            reference = item['highlights']\n        elif task == 'qa':\n            reference = item['answers']['text'][0] if item['answers']['text'] else \"No Answer\"\n        elif task == 'paraphrase':\n            reference = item['questions']['text'][1]\n        else:\n            raise ValueError(\"Unknown task\")\n\n        output = generate_output(tokenizer, model, prompt)\n        predictions.append(output.strip())\n        references.append(reference.strip())\n\n    return predictions, references\n\ndef jaccard_similarity(str1, str2):\n    set1, set2 = set(str1.split()), set(str2.split())\n    intersection = len(set1 & set2)\n    union = len(set1 | set2)\n    return intersection / union if union != 0 else 0.0\n\ndef compute_agreement_matrix(task, n_samples=100, split='validation', finetuned=False):\n    model_keys = list(MODEL_MAP.keys())\n    all_preds = {}\n\n    for key in model_keys:\n        model_path = f\"{key}_{task}\" if finetuned else MODEL_MAP[key]\n        preds, _ = run_predictions(key, task, n_samples, split, finetuned)\n        all_preds[key] = preds\n\n    matrix = pd.DataFrame(index=model_keys, columns=model_keys, dtype=float)\n    for i, m1 in enumerate(model_keys):\n        for j, m2 in enumerate(model_keys):\n            if i == j:\n                matrix.loc[m1, m2] = 1.0\n            elif pd.isna(matrix.loc[m1, m2]):\n                similarities = [jaccard_similarity(p1, p2) for p1, p2 in zip(all_preds[m1], all_preds[m2])]\n                score = round(np.mean(similarities), 4)\n                matrix.loc[m1, m2] = score\n                matrix.loc[m2, m1] = score\n    return matrix\n\ndef compute_baseline_scores(task, n_samples=100, split='validation'):\n    model_keys = list(MODEL_MAP.keys())\n    scores = {}\n\n    for key in model_keys:\n        preds, refs = run_predictions(key, task, n_samples, split)\n        if task == 'summarization':\n            score = evaluate_summarization(preds, refs)\n        elif task == 'qa':\n            score = evaluate_qa(preds, refs)\n        elif task == 'paraphrase':\n            score = evaluate_paraphrase(preds, refs)\n        else:\n            raise ValueError(\"Unknown task\")\n        scores[key] = round(score, 4)\n\n    return pd.Series(scores, name=f'{task}_baseline_scores')\n\ndef finetune_model_on_eval_split(model_key, task, n_samples=100, output_dir=\"finetuned_model\", split='validation'):\n    dataset_name, config = dataset_map[task]\n    dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n    tokenizer, base_model = AutoTokenizer.from_pretrained(MODEL_MAP[model_key], use_fast=True), MODEL_MAP[model_key]\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n        device_map=\"auto\"\n    )\n    model = prepare_model_for_kbit_training(model)\n\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n\n    def preprocess(example):\n        prompt = format_prompt(model_key, task, example)\n\n        if task == 'summarization':\n            reference = example['highlights']\n        elif task == 'qa':\n            reference = example['answers']['text'][0] if example['answers']['text'] else \"No Answer\"\n        elif task == 'paraphrase':\n            reference = example['questions']['text'][1]\n        else:\n            raise ValueError(\"Unknown task\")\n\n        full_input = prompt + \"\\n\" + reference\n        tokenized = tokenizer(full_input, truncation=True, padding=\"max_length\", max_length=512)\n        tokenized[\"labels\"] = tokenized[\"input_ids\"]\n        return tokenized\n\n    tokenized_dataset = dataset.map(preprocess, batched=False)\n    split = tokenized_dataset.train_test_split(test_size=0.2)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=2,\n        num_train_epochs=3,\n        eval_strategy=\"epoch\",\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=1,\n        save_strategy=\"epoch\",\n        report_to='none',\n        fp16=True,\n        load_best_model_at_end=True,\n        push_to_hub=False,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=split['train'],\n        eval_dataset=split['test'],\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n        # compute_metrics=compute_metric\n    )\n\n    trainer.train()\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\ndef compute_finetuned_scores(task, n_samples=100, split='validation'):\n    model_keys = list(MODEL_MAP.keys())\n    scores = {}\n\n    for key in model_keys:\n        model_path = f\"{key}_{task}\"\n        if not os.path.exists(model_path):\n            print(f\"Finetuned model not found at {model_path}, skipping...\")\n            continue\n        print(model_path, 'loaded')\n\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\",\n            quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n        )\n\n        dataset_name, config = dataset_map[task]\n        dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n\n        predictions, references = [], []\n\n        for item in tqdm(dataset, desc=f\"Evaluating {key}_{task}\"):\n            prompt = format_prompt(key, task, item)\n\n            if task == 'summarization':\n                reference = item['highlights']\n            elif task == 'qa':\n                reference = item['answers']['text'][0] if item['answers']['text'] else \"No Answer\"\n            elif task == 'paraphrase':\n                reference = item['questions']['text'][1]\n            else:\n                raise ValueError(\"Unknown task\")\n\n            output = generate_output(tokenizer, model, prompt)\n            predictions.append(output.strip())\n            references.append(reference.strip())\n\n        if task == 'summarization':\n            score = evaluate_summarization(predictions, references)\n        elif task == 'qa':\n            score = evaluate_qa(predictions, references)\n        elif task == 'paraphrase':\n            score = evaluate_paraphrase(predictions, references)\n        else:\n            raise ValueError(\"Unknown task\")\n\n        scores[key] = round(score, 4)\n\n    return pd.Series(scores, name=f'{task}_finetuned_scores')\n\ndef compute_inference_time_per_query(task, n_samples=10, finetuned=True, split='validation'):\n    model_keys = list(MODEL_MAP.keys())\n    times = {}\n\n    for key in model_keys:\n        model_path = f\"{key}_{task}\" if finetuned else MODEL_MAP[key]\n        if finetuned and not os.path.exists(model_path):\n            print(f\"Finetuned model not found at {model_path}, skipping...\")\n            continue\n        print(model_path, 'loaded')\n\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\",\n            quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n        )\n\n        dataset_name, config = dataset_map[task]\n        dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n\n        total_time = 0\n\n        for item in tqdm(dataset, desc=f\"Measuring inference time: {key}_{task}\"):\n            prompt = format_prompt(model_key, task, item)\n\n            if task == 'summarization':\n                reference = item['highlights']\n            elif task == 'qa':\n                reference = item['answers']['text'][0] if item['answers']['text'] else \"No Answer\"\n            elif task == 'paraphrase':\n                reference = item['questions']['text'][1]\n            else:\n                raise ValueError(\"Unknown task\")\n\n            start_time = time.perf_counter()\n            _ = generate_output(tokenizer, model, prompt)\n            end_time = time.perf_counter()\n\n            total_time += (end_time - start_time)\n\n        avg_time = total_time / n_samples\n        times[key] = round(avg_time, 4)\n        logger.info(f\"Avg inference time for {key} ({'finetuned' if finetuned else 'base'}) on {task}: {avg_time:.4f} seconds\")\n\n    return pd.Series(times, name=f\"{task}_{'finetuned' if finetuned else 'base'}_inference_time\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:01:19.759818Z","iopub.execute_input":"2025-05-09T20:01:19.760211Z","iopub.status.idle":"2025-05-09T20:01:49.581512Z","shell.execute_reply.started":"2025-05-09T20:01:19.760180Z","shell.execute_reply":"2025-05-09T20:01:49.580950Z"}},"outputs":[{"name":"stderr","text":"2025-05-09 20:01:32.402920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746820892.627656      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746820892.697649      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(compute_baseline_scores('summarization', 100))\nprint(compute_baseline_scores('qa', 100))\nprint(compute_baseline_scores('paraphrase', 10, 'train'))","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-09T09:43:57.897351Z","iopub.execute_input":"2025-05-09T09:43:57.898143Z","iopub.status.idle":"2025-05-09T09:47:02.874151Z","shell.execute_reply.started":"2025-05-09T09:43:57.898119Z","shell.execute_reply":"2025-05-09T09:47:02.873248Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 10%|█         | 1/10 [00:12<01:48, 12.05s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 20%|██        | 2/10 [00:18<01:08,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 30%|███       | 3/10 [00:30<01:10, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 40%|████      | 4/10 [00:42<01:05, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 50%|█████     | 5/10 [00:43<00:37,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 60%|██████    | 6/10 [00:46<00:24,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 70%|███████   | 7/10 [00:47<00:12,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 80%|████████  | 8/10 [00:51<00:08,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n 90%|█████████ | 9/10 [01:02<00:06,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n100%|██████████| 10/10 [01:14<00:00,  7.48s/it]\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 10%|█         | 1/10 [00:07<01:08,  7.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 20%|██        | 2/10 [00:13<00:52,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 30%|███       | 3/10 [00:15<00:31,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 40%|████      | 4/10 [00:22<00:33,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 50%|█████     | 5/10 [00:25<00:22,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 60%|██████    | 6/10 [00:32<00:22,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 70%|███████   | 7/10 [00:35<00:13,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 80%|████████  | 8/10 [00:42<00:10,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 90%|█████████ | 9/10 [00:49<00:06,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n100%|██████████| 10/10 [00:55<00:00,  5.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"qwen     0.8503\nopt      1.6967\nllama    0.7213\nName: paraphrase_baseline_scores, dtype: float64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"finetune_model_on_eval_split('qwen', 'summarization', 100, 'qwen_summarization')\nfinetune_model_on_eval_split('qwen', 'qa', 100, 'qwen_qa')\nfinetune_model_on_eval_split('qwen', 'paraphrase', 100, 'qwen_paraphrase', 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:30:23.887417Z","iopub.execute_input":"2025-05-09T16:30:23.888187Z","iopub.status.idle":"2025-05-09T16:39:08.072114Z","shell.execute_reply.started":"2025-05-09T16:30:23.888156Z","shell.execute_reply":"2025-05-09T16:39:08.071220Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c93d360d64043beb874fad7f057ac82"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:52, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.584600</td>\n      <td>2.609855</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.524800</td>\n      <td>2.577319</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.521700</td>\n      <td>2.568787</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b04b143d00ab4705879d7113deaa74aa"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.501000</td>\n      <td>1.659271</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.603300</td>\n      <td>1.536844</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.641700</td>\n      <td>1.493082</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9551a3d0992244b28f375c1d96321209"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.305900</td>\n      <td>3.630843</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.099200</td>\n      <td>3.217761</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.803400</td>\n      <td>3.067207</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"finetune_model_on_eval_split('opt', 'summarization', 100, 'opt_summarization')\nfinetune_model_on_eval_split('opt', 'qa', 100, 'opt_qa')\nfinetune_model_on_eval_split('opt', 'paraphrase', 100, 'opt_paraphrase', 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:39:08.075946Z","iopub.execute_input":"2025-05-09T16:39:08.076225Z","iopub.status.idle":"2025-05-09T16:45:36.607643Z","shell.execute_reply.started":"2025-05-09T16:39:08.076205Z","shell.execute_reply":"2025-05-09T16:45:36.607028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5203ae5264ad4560811fa391c719422a"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.615900</td>\n      <td>2.633111</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.469500</td>\n      <td>2.601025</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.567500</td>\n      <td>2.591090</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b890d7c0ddef4401826a9af47d7dbc07"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 02:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.960400</td>\n      <td>1.971917</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.973400</td>\n      <td>1.787838</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.878700</td>\n      <td>1.720980</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622b3f7a69e344e89faa06911ae406b1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.185000</td>\n      <td>3.427264</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.087300</td>\n      <td>2.964802</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.614700</td>\n      <td>2.773673</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"finetune_model_on_eval_split('llama', 'summarization', 100, 'llama_summarization')\nfinetune_model_on_eval_split('llama', 'qa', 100, 'llama_qa')\nfinetune_model_on_eval_split('llama', 'paraphrase', 100, 'llama_paraphrase', 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:45:36.608680Z","iopub.execute_input":"2025-05-09T16:45:36.608932Z","iopub.status.idle":"2025-05-09T16:51:42.217442Z","shell.execute_reply.started":"2025-05-09T16:45:36.608914Z","shell.execute_reply":"2025-05-09T16:51:42.216597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82669ba113c461c955b27a1b6ad29ce"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.019300</td>\n      <td>3.068075</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.901600</td>\n      <td>2.981763</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.867800</td>\n      <td>2.955802</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b41298f173f5457c8552b2ea4f04e1b7"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:52, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.251600</td>\n      <td>2.413560</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.193100</td>\n      <td>2.144872</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.165900</td>\n      <td>2.050608</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fb057b1058442284615840a38c3409"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/206709020.py:219: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 01:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.243200</td>\n      <td>4.671090</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.903700</td>\n      <td>4.103427</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.498600</td>\n      <td>3.921953</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!rm models.zip && zip models.zip -r .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(compute_finetuned_scores('summarization', 10))\nprint(compute_finetuned_scores('qa', 10))\nprint(compute_finetuned_scores('paraphrase', 10, 'train'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:53:26.987313Z","iopub.execute_input":"2025-05-09T16:53:26.987554Z","iopub.status.idle":"2025-05-09T17:01:21.462835Z","shell.execute_reply.started":"2025-05-09T16:53:26.987537Z","shell.execute_reply":"2025-05-09T17:01:21.462024Z"}},"outputs":[{"name":"stdout","text":"qwen_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating qwen_summarization: 100%|██████████| 10/10 [01:05<00:00,  6.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"opt_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating opt_summarization: 100%|██████████| 10/10 [01:34<00:00,  9.46s/it]\n","output_type":"stream"},{"name":"stdout","text":"llama_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating llama_summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  10%|█         | 1/10 [00:11<01:45, 11.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  20%|██        | 2/10 [00:30<02:06, 15.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  30%|███       | 3/10 [00:42<01:38, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  40%|████      | 4/10 [00:53<01:16, 12.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  50%|█████     | 5/10 [01:04<01:01, 12.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  60%|██████    | 6/10 [01:15<00:47, 11.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  70%|███████   | 7/10 [01:32<00:40, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  80%|████████  | 8/10 [01:39<00:22, 11.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization:  90%|█████████ | 9/10 [01:49<00:10, 11.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_summarization: 100%|██████████| 10/10 [02:05<00:00, 12.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"qwen     0.0628\nopt      0.0603\nllama    0.0542\nName: summarization_finetuned_scores, dtype: float64\nqwen_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating qwen_qa: 100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d7d2cd2fe6487c937021da87343bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba396afc8e3242a9bb3fe6af8940b650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd755f653a64161807d4c4db09fbfb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2db7e324b10c41f1aab01b5aec400c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4431a463b764ffaab96c254a65f5e36"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d793045ddc6349f791066c051eaf4ac9"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"opt_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating opt_qa: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"llama_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating llama_qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  10%|█         | 1/10 [00:00<00:02,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  20%|██        | 2/10 [00:00<00:02,  2.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  30%|███       | 3/10 [00:01<00:03,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  40%|████      | 4/10 [00:01<00:02,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  50%|█████     | 5/10 [00:03<00:03,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  60%|██████    | 6/10 [00:05<00:05,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  70%|███████   | 7/10 [00:05<00:03,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  80%|████████  | 8/10 [00:06<00:01,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa:  90%|█████████ | 9/10 [00:09<00:01,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_qa: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"qwen     0.4154\nopt      0.4154\nllama    0.4137\nName: qa_finetuned_scores, dtype: float64\nqwen_paraphrase loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating qwen_paraphrase: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d631b6c7dc4bd0894d7902e523c383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a47ab99f017042bf9fa1da33dcb76add"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"name":"stdout","text":"opt_paraphrase loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating opt_paraphrase: 100%|██████████| 10/10 [00:38<00:00,  3.89s/it]\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"llama_paraphrase loaded\n","output_type":"stream"},{"name":"stderr","text":"Evaluating llama_paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  10%|█         | 1/10 [00:01<00:15,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  20%|██        | 2/10 [00:02<00:10,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  30%|███       | 3/10 [00:03<00:08,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  40%|████      | 4/10 [00:04<00:05,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  50%|█████     | 5/10 [00:05<00:04,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  60%|██████    | 6/10 [00:06<00:03,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  70%|███████   | 7/10 [00:06<00:02,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  80%|████████  | 8/10 [00:08<00:01,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase:  90%|█████████ | 9/10 [00:10<00:01,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating llama_paraphrase: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n","output_type":"stream"},{"name":"stdout","text":"qwen     3.7943\nopt      2.9533\nllama    3.1807\nName: paraphrase_finetuned_scores, dtype: float64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import time\nprint(compute_inference_time_per_query('summarization', n_samples=10, finetuned=False))\nprint(compute_inference_time_per_query('qa', n_samples=10, finetuned=False))\nprint(compute_inference_time_per_query('paraphrase', n_samples=10, finetuned=False, split='train'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:52:12.653138Z","iopub.execute_input":"2025-05-09T11:52:12.653696Z","iopub.status.idle":"2025-05-09T12:10:28.100583Z","shell.execute_reply.started":"2025-05-09T11:52:12.653676Z","shell.execute_reply":"2025-05-09T12:10:28.099763Z"}},"outputs":[{"name":"stdout","text":"Qwen/Qwen2.5-1.5B loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: qwen_summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  10%|█         | 1/10 [00:00<00:03,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  20%|██        | 2/10 [00:02<00:13,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  30%|███       | 3/10 [00:12<00:35,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  40%|████      | 4/10 [00:12<00:18,  3.15s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  50%|█████     | 5/10 [00:47<01:14, 14.87s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  60%|██████    | 6/10 [00:48<00:39,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  70%|███████   | 7/10 [01:24<00:55, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  80%|████████  | 8/10 [01:24<00:25, 12.63s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization:  90%|█████████ | 9/10 [01:59<00:19, 19.69s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_summarization: 100%|██████████| 10/10 [02:35<00:00, 15.60s/it]","output_type":"stream"},{"name":"stdout","text":"facebook/opt-1.3b loaded\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c731ea571ea34557ad82bae6f3d3b937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63bae3e614e4be79af71f988009ea5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"049158c231f5404182f884cd9dfec525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ee16488f9d4eada51e3e660da2e9c0"}},"metadata":{}},{"name":"stderr","text":"Measuring inference time: opt_summarization: 100%|██████████| 10/10 [00:45<00:00,  4.51s/it]","output_type":"stream"},{"name":"stdout","text":"meta-llama/Llama-3.2-1B-Instruct loaded\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288f9fbdc268424c8306db3897423f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e72bb9dd72f4d7b992f3f8c8f2259b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c325b0169794da1b41ecdc66198178b"}},"metadata":{}},{"name":"stderr","text":"Measuring inference time: llama_summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  10%|█         | 1/10 [00:05<00:49,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  20%|██        | 2/10 [00:05<00:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  30%|███       | 3/10 [00:05<00:09,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  40%|████      | 4/10 [00:21<00:42,  7.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  50%|█████     | 5/10 [00:22<00:24,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  60%|██████    | 6/10 [00:39<00:35,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  70%|███████   | 7/10 [00:39<00:18,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization:  90%|█████████ | 9/10 [00:39<00:03,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_summarization: 100%|██████████| 10/10 [00:45<00:00,  4.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"qwen     15.5970\nopt       4.5124\nllama     4.5618\nName: summarization_base_inference_time, dtype: float64\nQwen/Qwen2.5-1.5B loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fe47b34c9644c7a08a57655eed01cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f1bd6a575f4572af2968aa13c3e241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7eab8e8367d4a01b0f4e344e88cff5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d2e143563941858dd7c7532a9a0217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85c870f0e844b41828ca0787bee2697"}},"metadata":{}},{"name":"stderr","text":"Measuring inference time: qwen_qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  10%|█         | 1/10 [00:00<00:04,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  20%|██        | 2/10 [00:00<00:03,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  30%|███       | 3/10 [00:01<00:03,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  40%|████      | 4/10 [00:02<00:03,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  50%|█████     | 5/10 [00:02<00:02,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  60%|██████    | 6/10 [00:26<00:34,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  70%|███████   | 7/10 [00:28<00:18,  6.25s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  80%|████████  | 8/10 [00:28<00:08,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa:  90%|█████████ | 9/10 [00:53<00:10, 10.74s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_qa: 100%|██████████| 10/10 [00:53<00:00,  5.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"facebook/opt-1.3b loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: opt_qa: 100%|██████████| 10/10 [03:20<00:00, 20.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"meta-llama/Llama-3.2-1B-Instruct loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: llama_qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  10%|█         | 1/10 [00:00<00:03,  2.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  20%|██        | 2/10 [00:00<00:02,  2.83it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  30%|███       | 3/10 [00:01<00:04,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  40%|████      | 4/10 [00:02<00:03,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  50%|█████     | 5/10 [00:05<00:07,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  60%|██████    | 6/10 [00:06<00:05,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  70%|███████   | 7/10 [00:07<00:03,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  80%|████████  | 8/10 [00:23<00:12,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa:  90%|█████████ | 9/10 [00:25<00:04,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_qa: 100%|██████████| 10/10 [00:27<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"qwen      5.3988\nopt      20.0835\nllama     2.7093\nName: qa_base_inference_time, dtype: float64\nQwen/Qwen2.5-1.5B loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: qwen_paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  10%|█         | 1/10 [00:36<05:26, 36.25s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  20%|██        | 2/10 [00:58<03:45, 28.21s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  30%|███       | 3/10 [01:34<03:42, 31.78s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  40%|████      | 4/10 [02:10<03:19, 33.30s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  50%|█████     | 5/10 [02:45<02:50, 34.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  60%|██████    | 6/10 [02:49<01:35, 23.81s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  70%|███████   | 7/10 [02:57<00:55, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  80%|████████  | 8/10 [03:09<00:32, 16.35s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase:  90%|█████████ | 9/10 [03:44<00:22, 22.34s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nMeasuring inference time: qwen_paraphrase: 100%|██████████| 10/10 [03:52<00:00, 23.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"facebook/opt-1.3b loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: opt_paraphrase: 100%|██████████| 10/10 [02:45<00:00, 16.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"meta-llama/Llama-3.2-1B-Instruct loaded\n","output_type":"stream"},{"name":"stderr","text":"Measuring inference time: llama_paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  10%|█         | 1/10 [00:16<02:27, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  20%|██        | 2/10 [00:32<02:10, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  30%|███       | 3/10 [00:48<01:52, 16.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  40%|████      | 4/10 [01:04<01:36, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  50%|█████     | 5/10 [01:07<00:56, 11.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  60%|██████    | 6/10 [01:23<00:51, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  70%|███████   | 7/10 [01:26<00:28,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  80%|████████  | 8/10 [01:33<00:18,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase:  90%|█████████ | 9/10 [01:50<00:11, 11.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nMeasuring inference time: llama_paraphrase: 100%|██████████| 10/10 [02:06<00:00, 12.65s/it]","output_type":"stream"},{"name":"stdout","text":"qwen     23.2030\nopt      16.5447\nllama    12.6517\nName: paraphrase_base_inference_time, dtype: float64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"compute_agreement_matrix('summarization', 10, 'validation', True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:18:01.888536Z","iopub.execute_input":"2025-05-09T17:18:01.889181Z","iopub.status.idle":"2025-05-09T17:20:39.355921Z","shell.execute_reply.started":"2025-05-09T17:18:01.889159Z","shell.execute_reply":"2025-05-09T17:20:39.355130Z"}},"outputs":[{"name":"stdout","text":"qwen_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:52<00:00,  5.28s/it]\n","output_type":"stream"},{"name":"stdout","text":"opt_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:22<00:00,  2.28s/it]\n","output_type":"stream"},{"name":"stdout","text":"llama_summarization loaded\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 10%|█         | 1/10 [00:06<00:59,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 20%|██        | 2/10 [00:15<01:03,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 30%|███       | 3/10 [00:22<00:51,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 40%|████      | 4/10 [00:28<00:40,  6.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 50%|█████     | 5/10 [00:33<00:31,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 60%|██████    | 6/10 [00:41<00:26,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 70%|███████   | 7/10 [00:49<00:22,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 80%|████████  | 8/10 [00:53<00:12,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 90%|█████████ | 9/10 [01:00<00:06,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n100%|██████████| 10/10 [01:04<00:00,  6.48s/it]\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"         qwen     opt   llama\nqwen   1.0000  0.9083  0.8840\nopt    0.9083  1.0000  0.8956\nllama  0.8840  0.8956  1.0000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qwen</th>\n      <th>opt</th>\n      <th>llama</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>qwen</th>\n      <td>1.0000</td>\n      <td>0.9083</td>\n      <td>0.8840</td>\n    </tr>\n    <tr>\n      <th>opt</th>\n      <td>0.9083</td>\n      <td>1.0000</td>\n      <td>0.8956</td>\n    </tr>\n    <tr>\n      <th>llama</th>\n      <td>0.8840</td>\n      <td>0.8956</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"compute_agreement_matrix('qa', 10, 'validation', True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:20:39.357169Z","iopub.execute_input":"2025-05-09T17:20:39.357373Z","iopub.status.idle":"2025-05-09T17:21:07.880132Z","shell.execute_reply.started":"2025-05-09T17:20:39.357358Z","shell.execute_reply":"2025-05-09T17:21:07.879483Z"}},"outputs":[{"name":"stdout","text":"qwen_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:04<00:00,  2.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"opt_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:02<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"llama_qa loaded\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 10%|█         | 1/10 [00:00<00:01,  6.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 20%|██        | 2/10 [00:00<00:01,  5.66it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 30%|███       | 3/10 [00:00<00:02,  2.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 40%|████      | 4/10 [00:01<00:02,  2.76it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 50%|█████     | 5/10 [00:01<00:02,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 60%|██████    | 6/10 [00:03<00:03,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 70%|███████   | 7/10 [00:04<00:02,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 80%|████████  | 8/10 [00:04<00:01,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 90%|█████████ | 9/10 [00:04<00:00,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"         qwen     opt   llama\nqwen   1.0000  0.9322  0.9902\nopt    0.9322  1.0000  0.9269\nllama  0.9902  0.9269  1.0000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qwen</th>\n      <th>opt</th>\n      <th>llama</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>qwen</th>\n      <td>1.0000</td>\n      <td>0.9322</td>\n      <td>0.9902</td>\n    </tr>\n    <tr>\n      <th>opt</th>\n      <td>0.9322</td>\n      <td>1.0000</td>\n      <td>0.9269</td>\n    </tr>\n    <tr>\n      <th>llama</th>\n      <td>0.9902</td>\n      <td>0.9269</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"compute_agreement_matrix('paraphrase', 10, 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T17:46:43.729257Z","iopub.execute_input":"2025-05-09T17:46:43.730447Z","iopub.status.idle":"2025-05-09T17:49:19.606597Z","shell.execute_reply.started":"2025-05-09T17:46:43.730412Z","shell.execute_reply":"2025-05-09T17:49:19.605973Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b2ff08f5cb4e71bcb4908369132411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quora.py:   0%|          | 0.00/2.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa335c05027e41e5b0d3d6b619333f50"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for quora contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/quora.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a89a59b0e384934be661130f30d6def"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/404290 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea59f106057c4a41ad8a53f51c12f124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1279a59b93ae417f99c16b91efc5d7c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc03c544a2a14cd5bb35ba55aac8d4ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf1eeabacb0b43b68c461e9facfad3c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d3c22c8dfb4f40bad40dce4437511a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a9701e90a404decb6dec536c99a5a35"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b92ddf0702c4f43935b76e010ade9a0"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0354b4bc0ac045ad8dbf6c23fba642e3"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 10/10 [00:22<00:00,  2.30s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda1b687c06a4ead9fc7d43663767da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7204e1f0cb847bd9ae04b2d30b34ceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9adb4dbf597240c697b8e9f1167e3271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d9eaec9af74f48928cc32cfeaec7ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/221 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d3737050ab4be39a463ecb09d54e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00fbe3df44543cdba36fe7f9926cd10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcebc4c7cda04197aff16041aeb6633b"}},"metadata":{}},{"name":"stderr","text":"\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n 10%|█         | 1/10 [00:01<00:10,  1.14s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:11,  1.39s/it]\u001b[A\n 30%|███       | 3/10 [00:05<00:14,  2.08s/it]\u001b[A\n 40%|████      | 4/10 [00:06<00:10,  1.68s/it]\u001b[A\n 50%|█████     | 5/10 [00:09<00:10,  2.05s/it]\u001b[A\n 60%|██████    | 6/10 [00:11<00:07,  1.94s/it]\u001b[A\n 70%|███████   | 7/10 [00:12<00:05,  1.92s/it]\u001b[A\n 80%|████████  | 8/10 [00:13<00:03,  1.60s/it]\u001b[A\n 90%|█████████ | 9/10 [00:15<00:01,  1.64s/it]\u001b[A\n100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0cf2fde8b4644eda28af56c8fac2c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb294cf5a5d423992bed25d2a45e3d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7cc9f384f884c628064891a2f85ff20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0fec7749544c78b95a400ee411fa82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d658224c439741699fbbbb7e673d6377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55c453139e14e57b086f1af109fad56"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 10%|█         | 1/10 [00:15<02:23, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 20%|██        | 2/10 [00:17<00:59,  7.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 30%|███       | 3/10 [00:17<00:30,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 40%|████      | 4/10 [00:19<00:19,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 50%|█████     | 5/10 [00:20<00:11,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 60%|██████    | 6/10 [00:21<00:07,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 70%|███████   | 7/10 [00:21<00:04,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 80%|████████  | 8/10 [00:22<00:02,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n 90%|█████████ | 9/10 [00:24<00:01,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n100%|██████████| 10/10 [00:25<00:00,  2.59s/it]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         qwen     opt   llama\nqwen   1.0000  0.5547  0.5729\nopt    0.5547  1.0000  0.4916\nllama  0.5729  0.4916  1.0000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qwen</th>\n      <th>opt</th>\n      <th>llama</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>qwen</th>\n      <td>1.0000</td>\n      <td>0.5547</td>\n      <td>0.5729</td>\n    </tr>\n    <tr>\n      <th>opt</th>\n      <td>0.5547</td>\n      <td>1.0000</td>\n      <td>0.4916</td>\n    </tr>\n    <tr>\n      <th>llama</th>\n      <td>0.5729</td>\n      <td>0.4916</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"for v in dir():\n    exec('del '+ v)\n    del v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:31:05.661941Z","iopub.execute_input":"2025-05-09T18:31:05.662524Z","iopub.status.idle":"2025-05-09T18:31:05.675928Z","shell.execute_reply.started":"2025-05-09T18:31:05.662501Z","shell.execute_reply":"2025-05-09T18:31:05.675134Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def collaborative_inference(task, model_idx1, model_idx2, n_samples=100, finetuned=True, split='validation'):\n    keys = list(MODEL_MAP.keys())\n    m1, m2 = keys[model_idx1], keys[model_idx2]\n    path1 = f\"{m1}_{task}\" if finetuned else MODEL_MAP[m1]\n    path2 = f\"{m2}_{task}\" if finetuned else MODEL_MAP[m2]\n\n    if finetuned and (not os.path.exists(path1) or not os.path.exists(path2)):\n        logger.warning(\"One of the finetuned models is missing.\")\n        return\n\n    print(path1, path2)\n\n    tokenizer1 = AutoTokenizer.from_pretrained(path1)\n    tokenizer1.pad_token = tokenizer1.eos_token\n    model1 = AutoModelForCausalLM.from_pretrained(\n        path1,\n        device_map='auto',\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n    )\n\n    tokenizer2 = AutoTokenizer.from_pretrained(path2)\n    tokenizer2.pad_token = tokenizer2.eos_token\n    model2 = AutoModelForCausalLM.from_pretrained(\n        path2,\n        device_map='auto',\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n    )\n\n    dataset_name, config = dataset_map[task]\n    dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n\n    predictions, references = [], []\n\n    for item in tqdm(dataset, desc=f\"Collaborative inference: {m1} → {m2} on {task}\"):\n        prompt = format_prompt(m1, task, item)\n\n        if task == 'summarization':\n            reference = item['highlights']\n        elif task == 'qa':\n            reference = item['answers']['text'][0] if item['answers']['text'] else \"No Answer\"\n        elif task == 'paraphrase':\n            reference = item['questions']['text'][1]\n        else:\n            raise ValueError(\"Unknown task\")\n\n        # Step 1: First model generates output\n        initial_output = generate_output(tokenizer1, model1, prompt).strip()\n        initial_output[len(prompt):]\n        # Step 2: Second model refines based on initial output\n        # Format the second prompt using model2's formatting style\n        if task == 'summarization':\n            collab_item = {'article': f\"{item['article']}\\nInitial Summary: {initial_output}\"}\n        elif task == 'qa':\n            collab_item = {'context': item['context'], 'question': item['question'] + f\"\\nInitial Answer: {initial_output}\"}\n        elif task == 'paraphrase':\n            collab_item = {'questions': {'text': [item['questions']['text'][0] + f\"\\nInitial Paraphrase: {initial_output}\", \"\"]}}\n\n        collaboration_prompt = format_prompt(m2, task, collab_item)\n        final_output = generate_output(tokenizer2, model2, collaboration_prompt).strip()\n        final_output = final_output[len(collaboration_prompt):]\n        print(final_output)\n        predictions.append(initial_output.strip())\n        references.append(reference.strip())\n\n    # Evaluate\n    if task == 'summarization':\n        score = evaluate_summarization(predictions, references)\n    elif task == 'qa':\n        score = evaluate_qa(predictions, references)\n    elif task == 'paraphrase':\n        score = evaluate_paraphrase(predictions, references)\n    else:\n        raise ValueError(\"Unknown task\")\n\n    print(f\"Collaborative inference score for {m1} → {m2} on {task}: {round(score, 4)}\")\n    return round(score, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:56:01.659355Z","iopub.execute_input":"2025-05-09T18:56:01.660010Z","iopub.status.idle":"2025-05-09T18:56:01.669914Z","shell.execute_reply.started":"2025-05-09T18:56:01.659989Z","shell.execute_reply":"2025-05-09T18:56:01.669086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"score = collaborative_inference('summarization', model_idx1=1, model_idx2=2, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:49:21.988365Z","iopub.execute_input":"2025-05-09T19:49:21.989003Z","iopub.status.idle":"2025-05-09T19:53:52.538457Z","shell.execute_reply.started":"2025-05-09T19:49:21.988982Z","shell.execute_reply":"2025-05-09T19:53:52.537720Z"}},"outputs":[{"name":"stdout","text":"opt_summarization llama_summarization\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → llama on summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  10%|█         | 1/10 [00:27<04:05, 27.27s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* Zully Broussard selflessly gave one of her kidneys to a stranger, and it resulted in six patients receiving transplants.\n* The California Pacific Medical Center is using a computer program to match donors and recipients, taking it from a simple swapping principle to a much higher level.\n* The chain of surgeries is taking five surgeons, a covey of physician assistants, nurses, and anesthesiologists, and more than 40 support staff to perform the surgeries.\n* The chain of surgeries is to be wrapped up on Friday, with the last donor giving a kidney to someone who has been biding time on a deceased donor list to complete the chain.\n* The process of matching donors and recipients is taking about three to four months, compared to the three weeks it took in the past.\n* The computer program, created by David Jacobs, has the potential to open up possibilities for pairing compatible donors and recipients.\n* The significance of the altruistic donor is that it opens up possibilities for matching donors and recipients, and has been able to match up 140 options for matching donors and recipients.\n* The chain of surgeries was a success, with six patients receiving transplants, and the chain is to be wrapped up on Friday.\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  20%|██        | 2/10 [00:55<03:40, 27.57s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* The MLS is set to mark the beginning of its 20th season on Saturday.\n* The league has grown from 10 teams in 1996 to 20 in 2015.\n* The league is set to add four new teams in 2020.\n* The new season is the first of a new domestic TV and media rights deal with FOX, ESPN, and Univision worth $700 million over eight years.\n* The salary cap restricts the amount teams can spend on playing squads, with each team having a number of spaces that can be allocated to \"off budget\" signings.\n* The league has seen significant growth in attendance and player development, with average attendances increasing from 31,683 in 1996 to 60,000 in 2019.\n* The league has attracted a large following in the US, with World Cup winners Kaka and David Villa representing the league.\n* The league has seen a significant increase in revenue, with the new season's domestic TV and media rights deal worth $700 million over eight years.\n* The league has faced criticism over its complex player acquisition rulebook, which restricts the amount teams can spend on playing squads.\n* The league has seen a significant increase in player\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  30%|███       | 3/10 [01:10<02:35, 22.19s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* French striker Bafetimbi Gomis collapsed during Swansea's 3-2 loss at Tottenham in the Premier League.\n* He was taken to hospital after collapsing in the first half at White Hart Lane.\n* He was wearing an oxygen mask during treatment.\n* Swansea tweeted that Gomis was \"fine\" after the match, with manager Garry Monk saying he was \"feeling well\".\n* Gomis had similar fainting spells in France, which prompted his former club Lyon's president to warn of the risks.\n* He has scored two league goals for Swansea this season, mostly in a backup role.\n* He became the Welsh side's top striker when Wilfried Bony signed with Manchester City in January.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  40%|████      | 4/10 [01:41<02:32, 25.46s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* Rory McIlroy's second shot on the eighth hole of the WGC Cadillac Championship into a lake was a rare moment of frustration.\n* McIlroy pulled his second shot into a lake using a 3-iron, which he joked was a 60-70 yard shot.\n* He composed himself to finish the round with a second round of 70, leaving him one-under for the tournament and eight shots off the pace set by leader JB Holmes.\n* McIlroy's frustration with elements of his game was still clear, as he said \"I think every golfer feels it because I don't hit shots like the one I hit on 8 on the range.\"\n* McIlroy's performance was an improvement on last week's performance at the Honda Classic event, where he failed to make the cut.\n* Ryan Holmes scored a two-under-par 71 to remain in second position overall, two shots behind Holmes.\n* Former world No 1. Adam Scott carded an impressive 68 to finish the day three shots off the pace at six-under.\n* Bubba Watson and Henrik Stenson are tied for fourth on four-under.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  50%|█████     | 5/10 [02:00<01:56, 23.36s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nHere are the highlights from the article:\n\n* An 8th-grade student, Cayman Naib, has gone missing in Pennsylvania.\n* He was last seen wearing a gray down winter jacket, black ski pants, and hiking boots.\n* His parents, Farid and Becky Naib, are searching for him and have set up a Facebook group, \"Find Cayman\".\n* Hundreds of volunteers have helped search for Cayman, including passing out fliers and canvassing areas.\n* Weather has limited search efforts, with rain and snow hindering efforts on Wednesday and Thursday.\n* Cayman's phone was out of power when he left school, and his friends have not been able to reach him.\n* The Naib family has posted on social media, saying they are worried about Cayman's safety and are saying \"Cayman, if you read this please know that you are forgiven for everything, and I mean everything, you have the ultimate free pass. Just come home, we are so worried about you\".\n* The search efforts will continue, with advanced tracking software and the deployment of the Civil Air Patrol planned for Sunday.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  60%|██████    | 6/10 [02:46<02:03, 30.80s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nMy vote for Father of the Year goes to Curt Schilling.\nThe former Major League Baseball pitcher recently fired off a series of fastballs and mowed down a group of Twitter trolls who made the mistake of tweeting vulgar and sexually-explicit comments about Schilling's teenage daughter.\nThe drama started, innocently enough, on February 25, when Schilling played the role of a proud father.\nHe sent a tweet congratulating his daughter, Gabby, on being accepted to Salve Regina University, where she'll play softball.\nIt read: \"Congrats to Gabby Schilling who will pitch for the Salve Regina Seahawks next year!! — Curt Schilling (@gehrig38)\"\nAlmost immediately, responses came in from young men, complete strangers who apparently followed Schilling on Twitter.\nThe tweets quickly went from immature, to creepy, to repugnant.\nThreats of rape were common.\nThe tweets were deleted, and the accounts were closed after this story went viral.\nBut not before Schilling captured some of the images and posted them on his blog.\nWhat was said about 17-year-old Gabby Schilling wasn't just obnoxious.\nIt was vile and obscene.\nWhat was said wasn't just mean and ugly.\nIt was threatening and scary.\nAs\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  70%|███████   | 7/10 [03:09<01:25, 28.47s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights of the article:\n\n* Two American women, aged 21 and 25, have been arrested for carving their initials into a wall at the Colosseum in Rome.\n* The women, from California, were spotted by fellow tourists, who then told security about the act.\n* The two letters \"J\" and \"N\" were scratched on a brick wall at the historic Roman amphitheater.\n* The women may face a fine for \"aggravated damage\" on a building of historical and artistic interest.\n* If one Russian's experience is anything to go by, the price won't be cheap.\n* The incident is not the first time that tourists have been caught carving graffiti on Rome's Colosseum.\n* Last November, a Russian tourist was fined and given a four-month suspended sentence for carving his name into the landmark.\n* The incident is also not the first time that tourists have been caught carving graffiti on other World Heritage Sites, including Machu Picchu in Peru and Angkor Archeological Park in Cambodia.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  80%|████████  | 8/10 [03:30<00:52, 26.00s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nHere are the highlights from the article:\n\n* Prince and 3rdEyeGirl are bringing the Hit & Run Tour to the US for the first time.\n* The tour will feature Prince, 3rdEyeGirl drummer Hannah Welton, and will be held in Louisville, Kentucky.\n* Tickets will go on sale on Monday, March 9 at 10 a.m. local time.\n* The tour will be the first time Prince has toured in the US since 2014.\n* The concert venues will be revealed via Twitter prior to each show.\n* A portion of the ticket sales will be donated to various Louisville charities.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  90%|█████████ | 9/10 [03:53<00:25, 25.01s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights of the article:\n\n* A shooting at a bar in Mali killed 5 people, including 1 French and 1 Belgian citizen, and injured 8 others.\n* Authorities called the shooting a \"criminal and terrorist act\" and attributed it to al-Murabitun, a North African jihadist group.\n* The group claimed responsibility for the attack in an audio message, stating it was in retaliation for the killing of one of its leaders.\n* The Malian government said it is committed to seeking peace and will not be intimidated by extremist groups.\n* A power struggle in northern Mali led to the takeover of the region by Tuareg fighters, who later turned to Islamist radicals.\n* Malian forces have battled various rebel factions, mostly in the northern region, with the help of French and African forces.\n* The attack is the latest in a series of violent attacks in Mali, which plunged the country into chaos after soldiers staged a coup three years ago.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization: 100%|██████████| 10/10 [04:15<00:00, 25.53s/it]","output_type":"stream"},{"name":"stdout","text":"d_header_id|>\nHere are the highlights from the article:\n\n* Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse have been charged by the Football Association for allegedly spitting during an altercation in a Premier League game.\n* The incident occurred in the 38th minute of the game at St James' Park.\n* The players have until 6pm GMT on Friday to respond to the charge.\n* The charge is related to an alleged breach of FA Rule E1[a], which states that players must not spit at each other.\n* Both Evans and Cisse released statements after the incident, with Evans saying he did not spit at Cisse and Cisse saying he reacted to something unpleasant.\n* Former Liverpool midfielder Dietmar Hamann described the incident as \"disgusting\" and former Manchester United midfielder Paul Scholes said Jonny Evans is not a spitting player.\n* Ex-Liverpool player Steve McManaman said Cisse stands up and spits at Evans' neck, which he finds disgusting.\n* The incident has raised concerns about the behavior of professional footballers and the need for tougher punishments for those who engage in such behavior.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → llama on summarization: 0.06\nScore: 0.06\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"score = collaborative_inference('summarization', model_idx1=1, model_idx2=0, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T18:56:01.920926Z","iopub.execute_input":"2025-05-09T18:56:01.921488Z","iopub.status.idle":"2025-05-09T19:01:36.969430Z","shell.execute_reply.started":"2025-05-09T18:56:01.921471Z","shell.execute_reply":"2025-05-09T19:01:36.968666Z"}},"outputs":[{"name":"stdout","text":"opt_summarization qwen_summarization\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  10%|█         | 1/10 [00:22<03:26, 22.93s/it]","output_type":"stream"},{"name":"stdout","text":"Zully Broussard donated one of her kidneys to a stranger, resulting in six other people receiving transplants through a process involving matched donor pairs or chains. This is possible because of the use of genetic profiles from donor-recipient pairs and the creation of a program called MatchGrid developed by a computer programmer named David Jacobs. The success of this process was due to the generosity of Broussard as well as the matchmaking abilities of the computer program.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  20%|██        | 2/10 [00:31<01:57, 14.73s/it]","output_type":"stream"},{"name":"stdout","text":"MLS marks 20-year anniversary<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  30%|███       | 3/10 [00:56<02:15, 19.30s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n\n• French striker Bafetimbi Gomis collapsed during Swansea's 3-2 loss at Tottenham in the Premier League.\n\n• He spent the night in hospital as a precaution.\n\n• Gomis had similar fainting spells in France.\n\n• He was wearing an oxygen mask during the incident.\n\n• Gomis has scored two league goals for Swansea this season.\n\n• He became the Welsh side's top striker when Wilfried Bony signed with Manchester City in January.\n\n• Gomis was taken to the hospital almost exactly three years ago at White Hart Lane, where Fabrice Muamba collapsed after suffering a cardiac arrest.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  40%|████      | 4/10 [01:43<03:00, 30.03s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n• Rory McIlroy pulled a second shot into a lake during the WGC Cadillac Championship\n• The shot was considered a rare moment of frustration by the world's reigning No. 1 player\n• McIlroy played the offending shot with a 3-iron instead of a longer club\n• He jokingly stated that the club \"must have gone a good 60, 70 yards\"\n• McIlroy finished the round with a second-round score of 70, one under par, placing him one under par for the tournament\n• His frustration with certain aspects of his game was evident throughout the tournament\n• McIlroy expressed concern about hitting shots that feel different on the course compared to practice sessions\n• Ryan Holmes remained in second place after scoring a two-under-par 71\n• Adam Scott finished the day three shots off the pace at six-under, finishing with an impressive 68\n• Bubba Watson and Henrik Stenson tied for fourth place on four-under, with both players playing strong rounds<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  50%|█████     | 5/10 [02:04<02:14, 26.90s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n•\tAn 13-year-old boy named Cayman Naib disappeared from a school in Pennsylvania.\n•\tThe boy wore winter clothes when he was last seen but didn’t wear waterproof gear or take his backpack.\n•\tParents have set up a Facebook group to help find him.\n•\tSeveral hundred people have volunteered to help search for Cayman.\n•\tCayman’s school says he was upset about something sent home from school.\n•\tNo one knows what happened to him yet.\n•\tAuthorities are using advanced technology to try to locate him.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  60%|██████    | 6/10 [03:11<02:42, 40.52s/it]","output_type":"stream"},{"name":"stdout","text":"wasn't just mean and ugly. It was threatening and scary. As a parent, it's the kind of thing that makes you rethink your opposition to public caning as a logical punishment for such transgressions. These misogynistic cowards may have thought they could hide in the darkness of anonymity, the sort that many have come to expect from social media sites, where you feel free to be a despicable human being because, you think, no one will ever find out who you really are and hold you accountable for your words. If so, they thought wrong. They couldn't hide. They were found out, and they got the throttling they so richly deserved. Thanks to dad. According to Schilling, who made it his mission to track down these cretins and make sure those they associate with know who they really are, two people have already paid a price due to their tweets. One was a student disc jockey at a community college in New Jersey, who was suspended, and the other was a part-time ticket seller for the New York Yankees, who was fired. Concerned that this is an example of exactly the kind of cyberbullying that leads some teenagers to commit suicide, Schilling is also thinking about taking legal action against some of\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  70%|███████   | 7/10 [03:42<01:52, 37.53s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n- Two American women were arrested for carving their initials into a wall with a coin inside Rome's Colosseum.\n- The women, aged 21 and 25, were spotted carrying out the act by fellow tourists.\n- The two letters -- J and N -- were about eight inches in length and scratched on a brick wall.\n- The women, both from California, reportedly snapped a selfie of themselves with their initials before they were arrested.\n- The incident comes as a reminder that no world landmark is safe from the salacious urges of tourists -- no matter how sacred it might be to the locals.\n- The incident involves other instances of inappropriate tourist behavior, including carvings on famous landmarks around the world.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  80%|████████  | 8/10 [04:11<01:09, 34.58s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n\n- Prince and 3rdEyeGirl are bringing their Hit & Run Tour to the US for the first time.\n\n- The first scheduled show is in Louisville, Kentucky, where 3rdEyeGirl drummer Hannah Welton is from.\n\n- Tickets will go on sale on Wednesday, March 9 at 10 am local time.\n\n- Concert venues have not been announced yet.\n\n- The U.K. tour took place in 2014.\n\n- Ticket proceeds will be donated to various charities in Louisville.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization:  90%|█████████ | 9/10 [04:54<00:37, 37.45s/it]","output_type":"stream"},{"name":"stdout","text":"Summary:\n\nOn June 25, 2016, a shooting occurred at a bar frequented by expatriates in Mali’s capital city, Bamako. This incident resulted in the deaths of five individuals, including two French nationals and three Malian residents. Additionally, eight others were injured during the attack. \n\nThe police stated that the perpetrator used a semi-automatic rifle and a shotgun to carry out the violence. The attack was identified as a \"criminal and terrorist act\" by the authorities.\n\nIn response to the event, the Malian government emphasized their commitment to maintaining peace and stability within the country. Meanwhile, a North African extremist group known as Al-Murabitun claimed responsibility for the attack via an audiotape, stating it was retaliatory against the death of their leader.\n\nThe United States recognized the threat posed by Al-Murabitun, describing them as a newly-formed militant organization operating in northern Mali. \n\nAdditionally, French officials condemned the attack and urged citizens to remain vigilant while traveling abroad. The French Foreign Ministry reported that the deceased individual from their country was aged 31.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on summarization: 100%|██████████| 10/10 [05:19<00:00, 31.97s/it]","output_type":"stream"},{"name":"stdout","text":"Highlights:\n\n- Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse charged by the Football Association.\n- Players had spat at each other during Wednesday night's Premier League game at St James' Park.\n- Charges involve breaking FA Rule E1[a], which states spitting at another player is not acceptable.\n- Players will have until 6 pm GMT on Friday to respond to charges.\n- Six-game bans possible if found guilty.\n- Evans says he didn't spit at Cisse, while Cisse reacts negatively afterward.\n- Former pundits Dietmar Hamann and Paul Scholes express disapproval of the incident.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → qwen on summarization: 0.06\nScore: 0.06\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"score = collaborative_inference('summarization', model_idx1=2, model_idx2=0, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:14:33.045142Z","iopub.execute_input":"2025-05-09T19:14:33.045435Z","iopub.status.idle":"2025-05-09T19:18:42.063352Z","shell.execute_reply.started":"2025-05-09T19:14:33.045415Z","shell.execute_reply":"2025-05-09T19:18:42.062524Z"}},"outputs":[{"name":"stdout","text":"llama_summarization qwen_summarization\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: llama → qwen on summarization:  10%|█         | 1/10 [00:27<04:06, 27.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Zully Broussard donated her kidney to a stranger, leading to a chain reaction of six transplants. Her generosity combined with big data processing, using genetic profiles to match donor pairs or chains quickly. This process allowed for more potential matches than previously possible, significantly expanding access to transplant opportunities. The chain of surgeries involves multiple doctors, including five surgeons, a team of support staff, and over 40 individuals working together.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  20%|██        | 2/10 [00:49<03:12, 24.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"MLS is now considered one of the major sports in America.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  30%|███       | 3/10 [01:00<02:08, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"User provided summary is already present in the text.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  40%|████      | 4/10 [01:17<01:46, 17.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Rory McIlroy pulls second shot into lake during WGC Cadillac Championship, leaves 8th hole with one-shot deficit to leader.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  50%|█████     | 5/10 [01:39<01:36, 19.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The highlight summary of the article can be as follows:\n\nAn 8th-grade student named Cayman Naib has gone missing in Pennsylvania. His parents have created a Facebook group to help find him. Volunteers have been helping with searches, while advanced technology is being used to locate him. The parents have asked Cayman to return home and apologize for his actions.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  60%|██████    | 6/10 [02:12<01:36, 24.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The article discusses the case of Curt Schilling, a former Major League Baseball pitcher, whose daughter Gabby was accepted to Salve Regina University. After Gabby was congratulated on her acceptance, several Twitter users began making derogatory comments about her. Despite the initial deletion of the tweets, Schilling managed to capture some of the images and posted them online. This led to widespread backlash and resulted in the suspension of a student DJ and the firing of a ticket seller. The incident highlights the dangers of cyberbullying and the importance of parental responsibility. Schilling's actions demonstrate his commitment to protecting his daughter and promoting online accountability.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  70%|███████   | 7/10 [02:40<01:16, 25.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The article discusses various instances of tourists engaging in inappropriate behavior while visiting famous landmarks around the world. It mentions several cases such as the American women carving their initials into a wall at the Colosseum in Rome, a Russian tourist being fined for vandalizing the Colosseum, a Russian porn flick being shot near the Pyramids of Giza and the Sphinx, and nude photos taken at a sacred site in Cambodia.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  80%|████████  | 8/10 [03:07<00:51, 25.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prince is coming to town! The Rock King (or should we say Princess?) is returning to the United States with his band, 3rdEyeGirl. This marks the first time he'll be touring the country. Their first performance together will take place in Louisville, Kentucky, home of 3rdEyeGirl's drummer, Hannah Welton. Tickets go on sale this Wednesday morning at 10 AM local time. Prince will also release his new album \"Rise\" this spring, which includes two hit songs. The proceeds from ticket sales will benefit several local charities in Louisville. Check out the official website for more information.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization:  90%|█████████ | 9/10 [03:39<00:27, 27.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Summary: In Bamako, Mali, a shooting occurred at a popular bar frequented by expatriates. Five people were killed, including three Malians and two foreigners. The attackers claimed responsibility for the attack, saying they did so as retribution against one of their own members. Al-Murabitun, a newly-formed militant group, is suspected of being behind the attack. The incident took place during a period of political instability in Mali where military coups led to civil wars involving Tuareg groups and Islamist radicals. The attack highlighted ongoing conflicts within the country.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on summarization: 100%|██████████| 10/10 [03:54<00:00, 23.42s/it]","output_type":"stream"},{"name":"stdout","text":"To generate more accurate results, please refine your question.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for llama → qwen on summarization: 0.0537\nScore: 0.0537\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"score = collaborative_inference('summarization', model_idx1=1, model_idx2=2, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:09:20.094933Z","iopub.execute_input":"2025-05-09T19:09:20.095462Z","iopub.status.idle":"2025-05-09T19:13:54.246893Z","shell.execute_reply.started":"2025-05-09T19:09:20.095440Z","shell.execute_reply":"2025-05-09T19:13:54.246217Z"}},"outputs":[{"name":"stdout","text":"opt_summarization llama_summarization\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → llama on summarization:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  10%|█         | 1/10 [00:24<03:40, 24.49s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* Zully Broussard selflessly gave one of her kidneys to a stranger, and it paired up with big data to result in six patients receiving transplants.\n* Her kidney was removed on Thursday, and went to a recipient who was paired with a donor.\n* The chain of surgeries is to be wrapped up on Friday, and includes five surgeons, a covey of physician assistants, nurses, and anesthesiologists, and over 40 support staff.\n* The chain of surgeries is taking place because of a computer program called MatchGrid, which matches donors and recipients based on genetic profiles.\n* The program has been used to match donors and recipients in previous cases, but the current chain is the largest and most complex yet.\n* Broussard's generosity is being recognized as a significant factor in the success of the transplant, and she has been praised for her altruism.\n* The medical center plans to hold a reception for all 12 patients on Friday to celebrate the success of the transplant.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  20%|██        | 2/10 [00:52<03:33, 26.70s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights of the article:\n\n* The first ever Major League Soccer (MLS) match was played on April 6, 1996, between San Jose Clash and DC United.\n* The match was attended by 31,683 fans at Spartan Stadium in San Jose, California.\n* The game was described by ESPN commentator Ty Keough as a \"momentous 'birth of a new era for American soccer'\".\n* The MLS has made significant progress since its inception, with attendance increasing to higher-than-ever levels and the number of teams growing to 20.\n* The league has also expanded its domestic TV and media rights deal with FOX, ESPN, and Univision worth $700 million over eight years.\n* The new season marks the first of a new domestic TV and media rights deal with FOX, ESPN, and Univision worth $700 million over eight years.\n* The league is expected to grow further, with the addition of four new teams in 2020.\n* The MLS has also made progress in attracting and retaining top players, with teams like Orlando City Soccer Club and New York City FC attracting world-class talent.\n* The league has also made efforts to improve its financial situation, with a reported $250 million loss in its first five years.\n* The MLS\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  30%|███       | 3/10 [01:13<02:47, 23.99s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* French striker Bafetimbi Gomis collapsed during Swansea's 3-2 loss at Tottenham in the Premier League on Wednesday.\n* He left the pitch conscious after about five minutes of treatment, wearing an oxygen mask.\n* Swansea tweeted that Gomis was \"fine\", with manager Garry Monk using the same word to describe his condition.\n* Gomis spent the night in hospital as a precaution, Swansea said on its website.\n* He was treated for low blood pressure, which causes fainting spells.\n* Gomis had similar fainting spells in France, prompting his former club Lyon to express worry about his health.\n* Swansea ran tests on Gomis before signing him on a free transfer last July.\n* Gomis has scored two league goals for Swansea this season, mostly in a backup role.\n* He became the Welsh side's top striker when Wilfried Bony signed with Manchester City in January.\n* Other footballers, including Fabrice Muamba and Marc-Vivien Foe, did not survive after collapsing on the pitch.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  40%|████      | 4/10 [01:45<02:42, 27.02s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* Rory McIlroy's second shot into a lake on the eighth hole of the WGC Cadillac Championship was a rare moment of frustration.\n* McIlroy felt the need to drop the ball and hit another shot, which he later joked was a \"3-iron for the rest of the round\".\n* He finished the round with a second round of 70, one-under for the tournament and eight shots off the pace set by leader JB Holmes.\n* McIlroy's frustration with his game was evident, with him saying \"I think every golfer feels it because I don't hit shots like the one I hit on 8 on the range\".\n* He also mentioned that he gets out on the course and hits shots that he's not seeing when he's in a more relaxed environment.\n* McIlroy's performance was an improvement on his last round at the Honda Classic, where he failed to make the cut.\n* Ryan Holmes scored a two-under-par 71 to remain in second position overall, two shots behind Holmes.\n* Former world No 1. Adam Scott carded an impressive 68 to finish the day three shots off the pace at six-under while Bubba Watson and Henrik Stenson are tied for fourth\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  50%|█████     | 5/10 [02:04<02:00, 24.09s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nHere are the highlights from the article:\n\n* An 8th-grade student, Cayman Naib, has gone missing in Pennsylvania.\n* He was last seen wearing a gray down winter jacket, black ski pants, and hiking boots.\n* His parents, Farid and Becky Naib, are searching for him in the Radnor-Wayne area, 20 miles from Philadelphia.\n* They believe Cayman left school upset after receiving an email from school about overdue home work.\n* His phone was out of power at the time he left school.\n* The parents have posted on a Facebook group called \"Find Cayman\" and have received help from hundreds of volunteers, including those who have passed out fliers and canvassed areas.\n* Weather has limited search efforts, with rain and snow hindering the search on Wednesday and Thursday.\n* The search will continue with the use of advanced, geo-spacial tracking software and the deployment of the Civil Air Patrol on Sunday.\n* The families have appealed to Cayman to return home, saying they are \"so worried about you\".<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  60%|██████    | 6/10 [02:49<02:04, 31.24s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nMy vote for Father of the Year goes to Curt Schilling. The former Major League Baseball pitcher recently fired off a series of fastballs and mowed down a group of Twitter trolls who made the mistake of tweeting vulgar and sexually-explicit comments about Schilling's teenage daughter. The drama started, innocently enough, on February 25, when Schilling played the role of a proud father. He sent a tweet congratulating his daughter, Gabby, on being accepted to Salve Regina University, where she'll play softball. It read: \"Congrats to Gabby Schilling who will pitch for the Salve Regina Seahawks next year!! — Curt Schilling (@gehrig38)\" Almost immediately, responses came in from young men, complete strangers who apparently followed Schilling on Twitter. The tweets quickly went from immature, to creepy, to repugnant. Threats of rape were common. The tweets were deleted, and the accounts were closed after this story went viral. But not before Schilling captured some of the images and posted them on his blog. What was said about 17-year-old Gabby Schilling wasn't just obnoxious. It was vile and obscene. What was said wasn't just mean and ugly. It was threatening and scary. As a\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  70%|███████   | 7/10 [03:15<01:28, 29.63s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* Two American women, aged 21 and 25, were arrested for carving their initials into a wall at Rome's Colosseum.\n* The women, from California, were spotted by fellow tourists who told security about the act.\n* The two letters, J and N, were about 8 inches in length and scratched on a brick wall.\n* The women were arrested and may face a fine for \"aggravated damage\" on a historical and artistic building.\n* This is not the first time tourists have been caught carving graffiti at Rome's Colosseum.\n* The incident comes after a 2014 incident where a Russian tourist was caught carving a letter \"K\" in a section of brickwork.\n* The women may also face a fine for \"aggravated damage\" at Egypt's Pyramids of Giza and the Sphinx.\n* The incident is also related to a Russian porn flick that was shot next to the Pyramids of Giza and the Sphinx.\n* Cambodia's Angkor Archeological Park has also experienced nudity-related incidents, including a 2017 incident where U.S. tourists were deported for taking partially nude photos at Preah Khan temple.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  80%|████████  | 8/10 [03:35<00:53, 26.59s/it]","output_type":"stream"},{"name":"stdout","text":"nd_header_id|>\nHere are the highlights from the article:\n\n* Prince and 3rdEyeGirl are touring the US for the first time.\n* The first show will take place in Louisville, Kentucky, the hometown of 3rdEyeGirl drummer Hannah Welton.\n* Tickets will go on sale Monday, March 9 at 10 a.m. local time.\n* The show will be a dual-venue event, with Prince crowning dual rock charts.\n* A venue has not been announced yet.\n* Portions of the ticket sales will be donated to various Louisville charities.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization:  90%|█████████ | 9/10 [03:59<00:25, 25.87s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHere are the highlights from the article:\n\n* A shooting at a bar popular with expatriates in Mali on Saturday killed 5 people, including 1 French and 1 Belgian citizen.\n* 1 French citizen, 1 Belgian citizen, and 3 Malians were killed in the attack in Bamako.\n* 8 people were wounded in the attack.\n* Authorities called the shooting a \"criminal and terrorist act\".\n* The government said Mali remains committed to seeking peace and will not be intimidated by extremist groups.\n* A North African jihadist group, al-Murabitun, claimed responsibility for the attack.\n* Al-Murabitun is considered a regional competitor to al-Qaeda in the Islamic Maghreb (AQIM).\n* The U.S. State Department said al-Murabitun is a \"newly-formed\" militant group.\n* French Foreign Minister Laurent Fabius said the victim from France was 31 years old.\n* French President Francois Hollande condemned the attack and U.S. Secretary of State John Kerry expressed condolences to the victims' families.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on summarization: 100%|██████████| 10/10 [04:20<00:00, 26.06s/it]","output_type":"stream"},{"name":"stdout","text":"d_header_id|>\nHere are the highlights from the article:\n\n* Manchester United defender Jonny Evans and Newcastle United striker Papiss Cisse have been charged by the Football Association for allegedly spitting during an altercation in a Premier League game.\n* The incident occurred in the 38th minute of the game, with both players spitting at each other.\n* The players have until 6pm GMT on Friday to respond to the charge.\n* If found guilty, both players could face six-game bans.\n* The charge is related to an alleged breach of FA Rule E1[a], which deals with spitting at another player.\n* Former Liverpool midfielder Dietmar Hamann described the incident as \"disgusting\" and said that the behaviour towards each other and the referee is deteriorating on a weekly basis.\n* Ex-Manchester United midfielder Paul Scholes said he did not believe Evans had deliberately spat at Cisse.\n* Former Liverpool player Steve McManaman described the incident as \"absolutely disgusting\" and said that Cisse stands up and spits at Evans' neck from about six inches.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → llama on summarization: 0.06\nScore: 0.06\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"score = collaborative_inference('qa', model_idx1=0, model_idx2=2, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:02:47.133886Z","iopub.execute_input":"2025-05-09T19:02:47.134167Z","iopub.status.idle":"2025-05-09T19:04:01.682199Z","shell.execute_reply.started":"2025-05-09T19:02:47.134127Z","shell.execute_reply":"2025-05-09T19:04:01.681356Z"}},"outputs":[{"name":"stdout","text":"qwen_qa llama_qa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b472b168d12d458faa93afc82665b669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7a05067dcb42dbaccda4623b9a8e17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef361acb76824507a693a04487ea8ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a20330e2cf4169bb0c775f76457a44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3334e3a663804d5db607132dcd3a3f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57961a428e9a4dc4adaf6e1a222dbb85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e7db318c3dd445c9ccf22a12ef23bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa7dd07ae264177a3d113da3c88eaa8"}},"metadata":{}},{"name":"stderr","text":"Collaborative inference: qwen → llama on qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  10%|█         | 1/10 [00:01<00:13,  1.54s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe Denver Broncos represented the AFC at Super Bowl 50.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  20%|██        | 2/10 [00:03<00:12,  1.60s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nCarolina Panthers represented the NFC at Super Bowl 50.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  30%|███       | 3/10 [00:04<00:10,  1.43s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nLevi's Stadium<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  40%|████      | 4/10 [00:05<00:08,  1.37s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe Denver Broncos won Super Bowl 50.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  50%|█████     | 5/10 [00:07<00:07,  1.48s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nGold was used to emphasize the 50th anniversary of the Super Bowl.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  60%|██████    | 6/10 [00:09<00:06,  1.61s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe theme of Super Bowl 50 was the \"Golden Anniversary\".<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  70%|███████   | 7/10 [00:11<00:05,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nFebruary 7, 2016<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  80%|████████  | 8/10 [00:12<00:03,  1.68s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nAFC stands for American Football Conference.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa:  90%|█████████ | 9/10 [00:13<00:01,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe golden anniversary<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on qa: 100%|██████████| 10/10 [00:15<00:00,  1.51s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nAFC stands for American Football Conference.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a6304fc01449e8b0ef54fb6bc5a22d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb9e6ab292b43f9b2eec739f82719bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36dda5b33480478d855f1e355e335f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1915a7f08547c2bd0f5ce20d23ce72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8b92618b304869bc6672b1d81bd90a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe86facb9554d33b10c91dbb455a556"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bca13180a9148b9be8da9a3498f5a07"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for qwen → llama on qa: 0.4077\nScore: 0.4077\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"score = collaborative_inference('qa', model_idx1=2, model_idx2=0, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:04:57.490452Z","iopub.execute_input":"2025-05-09T19:04:57.491028Z","iopub.status.idle":"2025-05-09T19:05:43.894916Z","shell.execute_reply.started":"2025-05-09T19:04:57.491006Z","shell.execute_reply":"2025-05-09T19:05:43.894203Z"}},"outputs":[{"name":"stdout","text":"llama_qa qwen_qa\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: llama → qwen on qa:  10%|█         | 1/10 [00:01<00:13,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Denver Broncos<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  20%|██        | 2/10 [00:04<00:17,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The Carolina Panthers represented the NFC at Super Bowl 50.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  30%|███       | 3/10 [00:08<00:21,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  40%|████      | 4/10 [00:09<00:14,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Denver Broncos<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  50%|█████     | 5/10 [00:10<00:09,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The gold color<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  60%|██████    | 6/10 [00:20<00:18,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the provided context, the theme of Super Bowl 50 was a \"golden anniversary\". This refers to the significance and importance placed on celebrating the 50th edition of the Super Bowl due to its special status and historical significance in NFL history.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  70%|███████   | 7/10 [00:22<00:11,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"February 7, 2016<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  80%|████████  | 8/10 [00:23<00:05,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"American Football Conference<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa:  90%|█████████ | 9/10 [00:28<00:03,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given context, the theme of Super Bowl 50 was the \"golden anniversary\".<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on qa: 100%|██████████| 10/10 [00:31<00:00,  3.11s/it]","output_type":"stream"},{"name":"stdout","text":"The answer is AFC, which stands for American Football Conference.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for llama → qwen on qa: 0.4027\nScore: 0.4027\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"score = collaborative_inference('qa', model_idx1=1, model_idx2=0, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:45:41.198682Z","iopub.execute_input":"2025-05-09T19:45:41.198945Z","iopub.status.idle":"2025-05-09T19:46:56.376481Z","shell.execute_reply.started":"2025-05-09T19:45:41.198930Z","shell.execute_reply":"2025-05-09T19:46:56.375685Z"}},"outputs":[{"name":"stdout","text":"opt_qa qwen_qa\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  10%|█         | 1/10 [00:02<00:23,  2.66s/it]","output_type":"stream"},{"name":"stdout","text":"The Denver Broncos represented the AFC at Super Bowl 50.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  20%|██        | 2/10 [00:04<00:15,  1.98s/it]","output_type":"stream"},{"name":"stdout","text":"Carolina Panthers<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  30%|███       | 3/10 [00:09<00:25,  3.63s/it]","output_type":"stream"},{"name":"stdout","text":"Based on the provided context, Super Bowl 50 took place at Levi's Stadium in the San Francisco Bay Area, specifically at Santa Clara, California.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  40%|████      | 4/10 [00:11<00:18,  3.05s/it]","output_type":"stream"},{"name":"stdout","text":"The Denver Broncos won Super Bowl 50.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  50%|█████     | 5/10 [00:12<00:11,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"gold<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  60%|██████    | 6/10 [00:15<00:10,  2.51s/it]","output_type":"stream"},{"name":"stdout","text":"The theme of Super Bowl 50 was the \"golden anniversary.\"<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  70%|███████   | 7/10 [00:18<00:07,  2.41s/it]","output_type":"stream"},{"name":"stdout","text":"February 7, 2016<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  80%|████████  | 8/10 [00:38<00:16,  8.26s/it]","output_type":"stream"},{"name":"stdout","text":"The AFC stands for American Football Conference.\n\nTo arrive at this answer:\n\n1. I first identified the relevant information from the given context. It mentions \"American Football Conference\" and \"National Football Conference.\"\n\n2. These are the two main divisions or conferences in American football leagues.\n\n3. The question asks specifically about the AFC, which matches perfectly with \"American Football Conference.\"\n\n4. Therefore, I concluded that AFC stands for American Football Conference based on the provided context.\n\nThis approach involves identifying key terms within the text and matching them to their corresponding definitions or abbreviations. In this case, \"AFC\" is directly defined by its description in the text.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa:  90%|█████████ | 9/10 [00:41<00:06,  6.61s/it]","output_type":"stream"},{"name":"stdout","text":"The theme of Super Bowl 50 was the \"golden anniversary.\"<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on qa: 100%|██████████| 10/10 [00:58<00:00,  5.85s/it]","output_type":"stream"},{"name":"stdout","text":"The acronym AFC stands for American Football Conference. This refers to one of two conferences in Major League Soccer and the NFL. AFC champions are from the American Football Conference.\n\nIn the context provided, it is mentioned that Super Bowl 50 was a championship game of the American Football Conference (AFC). The AFC champions were the Denver Broncos, who won against the NFC champion Carolina Panthers with a score of 24-10.\n\nSo, when referring to the American Football Conference, you can use the abbreviation AFC.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → qwen on qa: 0.4172\nScore: 0.4172\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"score = collaborative_inference('qa', model_idx1=1, model_idx2=2, n_samples=10, finetuned=True)\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:06:41.365162Z","iopub.execute_input":"2025-05-09T19:06:41.365445Z","iopub.status.idle":"2025-05-09T19:07:11.787643Z","shell.execute_reply.started":"2025-05-09T19:06:41.365426Z","shell.execute_reply":"2025-05-09T19:07:11.786898Z"}},"outputs":[{"name":"stdout","text":"opt_qa llama_qa\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → llama on qa:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  10%|█         | 1/10 [00:01<00:17,  1.97s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe American Football Conference (AFC) champion Denver Broncos represented the AFC at Super Bowl 50.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  20%|██        | 2/10 [00:03<00:13,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe Carolina Panthers were the NFC champions.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  30%|███       | 3/10 [00:04<00:09,  1.41s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nAt Santa Clara, California<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  40%|████      | 4/10 [00:05<00:07,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe Denver Broncos won Super Bowl 50.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  50%|█████     | 5/10 [00:07<00:07,  1.51s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe color used to emphasize the 50th anniversary of the Super Bowl was gold.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  60%|██████    | 6/10 [00:09<00:06,  1.55s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe theme of Super Bowl 50 was the \"golden anniversary\".<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  70%|███████   | 7/10 [00:10<00:04,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nFebruary 7, 2016<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  80%|████████  | 8/10 [00:11<00:02,  1.34s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe AFC stands for American Football Conference.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa:  90%|█████████ | 9/10 [00:13<00:01,  1.43s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nThe theme of Super Bowl 50 was the \"golden anniversary\".<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on qa: 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nAFC stands for American Football Conference.<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → llama on qa: 0.4172\nScore: 0.4172\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"score = collaborative_inference('paraphrase', model_idx1=0, model_idx2=2, n_samples=10, finetuned=True, split='train')\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:22:42.709732Z","iopub.execute_input":"2025-05-09T19:22:42.710001Z","iopub.status.idle":"2025-05-09T19:23:38.267016Z","shell.execute_reply.started":"2025-05-09T19:22:42.709981Z","shell.execute_reply":"2025-05-09T19:23:38.266372Z"}},"outputs":[{"name":"stdout","text":"qwen_paraphrase llama_paraphrase\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: qwen → llama on paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  10%|█         | 1/10 [00:03<00:30,  3.41s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\n<|im_start|>How to start investing in shares of Indian companies?<|im_end|><|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  20%|██        | 2/10 [00:06<00:24,  3.00s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat is the story of the Kohinoor diamond?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  30%|███       | 3/10 [00:10<00:25,  3.66s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\n<|im_start|>How does a VPN impact my internet speed?<|im_end|><|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  40%|████      | 4/10 [00:15<00:24,  4.01s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nHow do you feel about yourself? Is there anything that could be changed to make you happier?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  50%|█████     | 5/10 [00:19<00:20,  4.03s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhich one dissolves quickly in water among sugar, salt, methane and carbon dioxide?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  60%|██████    | 6/10 [00:26<00:21,  5.26s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nI'm a Capricorn with my sun in Capricorn, moon in Virgo, and rising in Capricorn. What does this mean for me?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  70%|███████   | 7/10 [00:28<00:12,  4.07s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nShould I buy Tiago?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  80%|████████  | 8/10 [00:32<00:07,  3.98s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\n<|im_start|>What specific steps can I take to develop my skills and knowledge in geology?<|im_end|><|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase:  90%|█████████ | 9/10 [00:35<00:03,  3.72s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhen should I use シ or し in Japanese?<|im_start|><|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: qwen → llama on paraphrase: 100%|██████████| 10/10 [00:38<00:00,  3.83s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nCan I access my Charter Motorola DCX3400?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1b4d5672a0443c9d643a5f146c3d08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e4dfb9783404c19baa40beed4174e11"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for qwen → llama on paraphrase: 1.9313\nScore: 1.9313\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"score = collaborative_inference('paraphrase', model_idx1=2, model_idx2=0, n_samples=10, finetuned=True, split='train')\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:24:15.030378Z","iopub.execute_input":"2025-05-09T19:24:15.030631Z","iopub.status.idle":"2025-05-09T19:25:28.691175Z","shell.execute_reply.started":"2025-05-09T19:24:15.030616Z","shell.execute_reply":"2025-05-09T19:25:28.690109Z"}},"outputs":[{"name":"stdout","text":"llama_paraphrase qwen_paraphrase\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: llama → qwen on paraphrase:  10%|█         | 1/10 [00:32<04:54, 32.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"across different sectors and companies to reduce risk.\n\n10. **Stay updated with news and events**: Stay informed about economic developments, regulatory changes, and other relevant information affecting the stock market.\n11. **Consider using financial advisors**: If necessary, consult a professional financial advisor who can provide guidance and advice tailored to your specific needs.\n\nRemember that investing involves risks, including loss of principal. It's important to do thorough research and seek expert advice before making any investment decisions.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  20%|██        | 2/10 [00:35<02:01, 15.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What is the history of the Kohinoor diamond?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  30%|███       | 3/10 [00:38<01:07,  9.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Is it possible to use a VPN and still have fast internet speeds?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  40%|████      | 4/10 [00:40<00:39,  6.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"How do you feel about being lonely?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  50%|█████     | 5/10 [00:42<00:24,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Which substances dissolve in water quickly?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  60%|██████    | 6/10 [00:48<00:21,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Astrology: I'm a Capricorn with a Moon in Cancer and an Ascendant in Virgo... what does this tell you about me?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  70%|███████   | 7/10 [00:50<00:12,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Do I need to get tiago?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  80%|████████  | 8/10 [00:53<00:07,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What are some ways to become a successful geologist?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  90%|█████████ | 9/10 [00:58<00:04,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"When should I use シ instead of し?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase: 100%|██████████| 10/10 [01:01<00:00,  6.17s/it]","output_type":"stream"},{"name":"stdout","text":"Can I hack my Charter Motorolla DCX3400?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for llama → qwen on paraphrase: 0.984\nScore: 0.984\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"score = collaborative_inference('paraphrase', model_idx1=1, model_idx2=0, n_samples=10, finetuned=True, split='train')\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:27:23.680012Z","iopub.execute_input":"2025-05-09T19:27:23.680872Z","iopub.status.idle":"2025-05-09T19:28:37.933887Z","shell.execute_reply.started":"2025-05-09T19:27:23.680847Z","shell.execute_reply":"2025-05-09T19:28:37.933090Z"}},"outputs":[{"name":"stdout","text":"opt_paraphrase qwen_paraphrase\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  10%|█         | 1/10 [00:03<00:31,  3.49s/it]","output_type":"stream"},{"name":"stdout","text":"How do I start investing in the stock market?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  20%|██        | 2/10 [00:07<00:30,  3.78s/it]","output_type":"stream"},{"name":"stdout","text":"What is the history behind the Kohinoor diamond?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  30%|███       | 3/10 [00:12<00:30,  4.30s/it]","output_type":"stream"},{"name":"stdout","text":"Can you help me improve the speed of my internet connection when using a virtual private network (VPN)?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  40%|████      | 4/10 [00:16<00:24,  4.05s/it]","output_type":"stream"},{"name":"stdout","text":"How do you feel about yourself? How can I help you?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  50%|█████     | 5/10 [00:21<00:21,  4.39s/it]","output_type":"stream"},{"name":"stdout","text":"What dissolves quickly in water? Salt, Sugar, Carbon Dioxide, Methane.<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  60%|██████    | 6/10 [00:26<00:19,  4.90s/it]","output_type":"stream"},{"name":"stdout","text":"I am a Capricorn Sun, Capricorn Moon, and Capricorn Rising... what does that mean for me?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  70%|███████   | 7/10 [00:28<00:11,  3.93s/it]","output_type":"stream"},{"name":"stdout","text":"Would you recommend Tiago?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  80%|████████  | 8/10 [00:31<00:07,  3.63s/it]","output_type":"stream"},{"name":"stdout","text":"What should I do to become a good geologist?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase:  90%|█████████ | 9/10 [00:58<00:10, 10.76s/it]","output_type":"stream"},{"name":"stdout","text":"When do you use シ instead of し?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → qwen on paraphrase: 100%|██████████| 10/10 [01:02<00:00,  6.28s/it]","output_type":"stream"},{"name":"stdout","text":"Is it possible to hack my Charter Motorolla DCX3400?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → qwen on paraphrase: 2.534\nScore: 2.534\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"score = collaborative_inference('paraphrase', model_idx1=1, model_idx2=2, n_samples=10, finetuned=True, split='train')\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:30:08.299508Z","iopub.execute_input":"2025-05-09T19:30:08.299780Z","iopub.status.idle":"2025-05-09T19:31:08.580217Z","shell.execute_reply.started":"2025-05-09T19:30:08.299760Z","shell.execute_reply":"2025-05-09T19:31:08.579458Z"}},"outputs":[{"name":"stdout","text":"opt_paraphrase llama_paraphrase\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: opt → llama on paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  10%|█         | 1/10 [00:02<00:26,  2.89s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat is the best way to start investing in the Indian stock market?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  20%|██        | 2/10 [00:06<00:25,  3.16s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat is the story of Kohinoor (Koh-i-Noor) Diamond?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  30%|███       | 3/10 [00:09<00:21,  3.11s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat can I do to boost the speed of my internet connection while I'm using a VPN?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  40%|████      | 4/10 [00:11<00:16,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhy am I feeling very lonely?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  50%|█████     | 5/10 [00:14<00:13,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhich one dissolves in water quickly?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  60%|██████    | 6/10 [00:16<00:10,  2.66s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat does the sign of Capricorn say about me?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  70%|███████   | 7/10 [00:17<00:06,  2.23s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nShould I buy tiago?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  80%|████████  | 8/10 [00:20<00:04,  2.19s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhat are the key skills and qualities of a good geologist?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase:  90%|█████████ | 9/10 [00:45<00:09,  9.37s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nWhen do you use シ instead of し?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: opt → llama on paraphrase: 100%|██████████| 10/10 [00:48<00:00,  4.81s/it]","output_type":"stream"},{"name":"stdout","text":"|end_header_id|>\nCan I hack my Charter Motorolla DCX3400?<|eot_id|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for opt → llama on paraphrase: 2.534\nScore: 2.534\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"score = collaborative_inference('paraphrase', model_idx1=2, model_idx2=0, n_samples=10, finetuned=True, split='train')\nprint(\"Score:\", score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:43:00.088181Z","iopub.execute_input":"2025-05-09T19:43:00.088856Z","iopub.status.idle":"2025-05-09T19:43:43.995649Z","shell.execute_reply.started":"2025-05-09T19:43:00.088835Z","shell.execute_reply":"2025-05-09T19:43:43.994786Z"}},"outputs":[{"name":"stdout","text":"llama_paraphrase qwen_paraphrase\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nCollaborative inference: llama → qwen on paraphrase:  10%|█         | 1/10 [00:03<00:31,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"How do I start investing in the Indian stock market?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  20%|██        | 2/10 [00:07<00:30,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Tell me about the history of Kohinoor (Koh-i-Noor) Diamond?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  30%|███       | 3/10 [00:11<00:26,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Is there any way to boost my internet speed when using a virtual private network?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  40%|████      | 4/10 [00:13<00:19,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"How do I feel about my mental health?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  50%|█████     | 5/10 [00:15<00:14,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Which substance dissolves in water most readily?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  60%|██████    | 6/10 [00:18<00:11,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What does this astrology tell you about yourself?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  70%|███████   | 7/10 [00:20<00:07,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Is it wise to purchase Tiago?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  80%|████████  | 8/10 [00:23<00:05,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"How do I become a good geologist?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase:  90%|█████████ | 9/10 [00:26<00:02,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"How can I say \"I am going to buy\" in Japanese?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"Collaborative inference: llama → qwen on paraphrase: 100%|██████████| 10/10 [00:29<00:00,  2.99s/it]","output_type":"stream"},{"name":"stdout","text":"Can you hack your Motorola Charter DCX3400?<|im_end|>\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Collaborative inference score for llama → qwen on paraphrase: 1.2639\nScore: 1.2639\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import time\n\ndef average_inference_time(task, model_idx1, model_idx2, n_samples=100, finetuned=True, split='validation'):\n    keys = list(MODEL_MAP.keys())\n    m1, m2 = keys[model_idx1], keys[model_idx2]\n    path1 = f\"{m1}_{task}\" if finetuned else MODEL_MAP[m1]\n    path2 = f\"{m2}_{task}\" if finetuned else MODEL_MAP[m2]\n\n    if finetuned and (not os.path.exists(path1) or not os.path.exists(path2)):\n        logger.warning(\"One of the finetuned models is missing.\")\n        return\n\n    tokenizer1 = AutoTokenizer.from_pretrained(path1)\n    tokenizer1.pad_token = tokenizer1.eos_token\n    model1 = AutoModelForCausalLM.from_pretrained(\n        path1,\n        device_map='auto',\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n    )\n\n    tokenizer2 = AutoTokenizer.from_pretrained(path2)\n    tokenizer2.pad_token = tokenizer2.eos_token\n    model2 = AutoModelForCausalLM.from_pretrained(\n        path2,\n        device_map='auto',\n        quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n    )\n\n    dataset_name, config = dataset_map[task]\n    dataset = load_dataset(dataset_name, config)[split].select(range(n_samples))\n    \n    times = []\n\n    for item in dataset:\n        prompt = format_prompt(m1, task, item)\n\n        if task == 'summarization':\n            reference = item['highlights']\n        elif task == 'qa':\n            reference = item['answers']['text'][0] if item['answers']['text'] else \"No Answer\"\n        elif task == 'paraphrase':\n            reference = item['questions']['text'][1]\n        else:\n            raise ValueError(\"Unknown task\")\n            \n        inputs = tokenizer1(prompt, return_tensors=\"pt\").to(model1.device)\n\n        start = time.time()\n        outputs1 = model1.generate(**inputs, max_new_tokens=50)\n        output_text1 = tokenizer1.decode(outputs1[0], skip_special_tokens=True)\n\n        inputs2 = tokenizer2(output_text1, return_tensors=\"pt\").to(model2.device)\n        outputs2 = model2.generate(**inputs2, max_new_tokens=50)\n        end = time.time()\n\n        times.append(end - start)\n\n    avg_time = np.mean(times)\n    logger.info(f\"Average inference time per query (task={task}, models={m1}+{m2}): {avg_time:.4f} seconds\")\n    return avg_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:05:35.169596Z","iopub.execute_input":"2025-05-09T20:05:35.169882Z","iopub.status.idle":"2025-05-09T20:05:35.179785Z","shell.execute_reply.started":"2025-05-09T20:05:35.169862Z","shell.execute_reply":"2025-05-09T20:05:35.178934Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"average_inference_time('summarization', 0, 2, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:05:35.804847Z","iopub.execute_input":"2025-05-09T20:05:35.805621Z","iopub.status.idle":"2025-05-09T20:07:29.108495Z","shell.execute_reply.started":"2025-05-09T20:05:35.805583Z","shell.execute_reply":"2025-05-09T20:07:29.107701Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"10.037115716934204"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"average_inference_time('summarization', 2, 0, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:08:47.978849Z","iopub.execute_input":"2025-05-09T20:08:47.979171Z","iopub.status.idle":"2025-05-09T20:10:53.502749Z","shell.execute_reply.started":"2025-05-09T20:08:47.979150Z","shell.execute_reply":"2025-05-09T20:10:53.501971Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"11.462097859382629"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"average_inference_time('qa', 0, 2, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:10:53.504031Z","iopub.execute_input":"2025-05-09T20:10:53.504573Z","iopub.status.idle":"2025-05-09T20:11:33.712685Z","shell.execute_reply.started":"2025-05-09T20:10:53.504554Z","shell.execute_reply":"2025-05-09T20:11:33.711901Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3209aeff29b94647869b47256907e80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dda1da056ed34b299a48436e8666cb1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185ec4e866844663bca4e4bd53babb60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9855876a683b4708b7f2964fa1792601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db97487c524447d695463518980debb2"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"2.794150233268738"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"average_inference_time('qa', 2, 0, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:11:33.713601Z","iopub.execute_input":"2025-05-09T20:11:33.713860Z","iopub.status.idle":"2025-05-09T20:13:04.938933Z","shell.execute_reply.started":"2025-05-09T20:11:33.713843Z","shell.execute_reply":"2025-05-09T20:13:04.938296Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"8.057641077041627"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"average_inference_time('paraphrase', 0, 2, 10, True, 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:17:56.036988Z","iopub.execute_input":"2025-05-09T20:17:56.037233Z","iopub.status.idle":"2025-05-09T20:18:32.460728Z","shell.execute_reply.started":"2025-05-09T20:17:56.037217Z","shell.execute_reply":"2025-05-09T20:18:32.459947Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"2.9082411766052245"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"average_inference_time('paraphrase', 2, 0, 10, True, 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T20:13:04.940533Z","iopub.execute_input":"2025-05-09T20:13:04.940733Z","iopub.status.idle":"2025-05-09T20:17:56.035619Z","shell.execute_reply.started":"2025-05-09T20:13:04.940718Z","shell.execute_reply":"2025-05-09T20:17:56.035049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f919b2ff29cd4796a53e010f39c8ab56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quora.py:   0%|          | 0.00/2.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d255b0528dd4799a98c58ebad65ed60"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for quora contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/quora.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/58.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bfde2a50db745d6862723b4fe1c064f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/404290 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"defe9c8aa85844bbbdd672b46b2c3235"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"5.968151974678039"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}